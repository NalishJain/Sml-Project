{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier, IsolationForest\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:, 1:-1].values\n",
    "x = np.array(x, dtype=float)\n",
    "y = data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting strings to numeric labels\n",
    "label = 0\n",
    "labels = {}\n",
    "inverse_labels = {}\n",
    "for i in set(y):\n",
    "    labels[i] = label\n",
    "    inverse_labels[label] = i\n",
    "    label += 1\n",
    "\n",
    "for i in range(len(y)):\n",
    "    y[i] = labels[y[i]]\n",
    "\n",
    "y = np.array(y, dtype=float)\n",
    "\n",
    "print(f\"Number of classes: {label}\")\n",
    "pd.DataFrame(y).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [\n",
    "    Integer(1, 20, name='pca_n_components'),\n",
    "    Integer(50, 500, name='gradient_boosting_n_estimators'),\n",
    "    Integer(2, 10, name='gradient_boosting_max_depth'),\n",
    "    Real(10**-4, 10**-1, name='gradient_boosting_learning_rate'),\n",
    "    Integer(50, 500, name='isolation_forest_n_estimators'),\n",
    "    Real(0.1, 0.5, name='isolation_forest_contamination'),\n",
    "    Integer(5, 100, name='hdbscan_min_samples'),\n",
    "    Integer(50, 500, name='hdbscan_min_cluster_size'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(pca_n_components, gradient_boosting_learning_rate, gradient_boosting_n_estimators, gradient_boosting_max_depth, isolation_forest_n_estimators, isolation_forest_contamination, hdbscan_min_samples, hdbscan_min_cluster_size):\n",
    "    #pre-processing\n",
    "    scaler = StandardScaler()\n",
    "    pca = PCA(n_components=pca_n_components)\n",
    "    x_processed = pca.fit_transform(scaler.fit_transform(x))\n",
    "\n",
    "    #outlier-detection\n",
    "    isolation_forest = IsolationForest(n_estimators=isolation_forest_n_estimators, contamination=isolation_forest_contamination)\n",
    "    isolation_forest.fit(x_processed)\n",
    "    x_transformed = x_processed[np.where(isolation_forest.predict(x_processed) != -1)[0]]\n",
    "\n",
    "    #clustering\n",
    "    clusterer = hdbscan.HDBSCAN(min_samples=hdbscan_min_samples, min_cluster_size=hdbscan_min_cluster_size)\n",
    "    cluster_labels = clusterer.fit_predict(x_transformed)\n",
    "    x_clustered = np.concatenate((x_transformed, cluster_labels.reshape(-1, 1)), axis=1)\n",
    "\n",
    "    #classification\n",
    "    gbm = GradientBoostingClassifier(n_estimators=gradient_boosting_n_estimators, learning_rate=gradient_boosting_learning_rate, max_depth=gradient_boosting_max_depth, random_state=42)\n",
    "    gbm.fit(x_clustered, y)\n",
    "\n",
    "    #accuracy\n",
    "    return -np.mean(cross_val_score(gbm, x_clustered, y, cv=5, n_jobs=1, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gp_minimize(objective, search_space, n_calls=50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best hyperparameters:', dict(zip(['min_samples_leaf', 'n_estimators', 'max_features'], result.x)))\n",
    "print('Best score:', -result.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=result.x[0])\n",
    "x_test_processed = pca.fit_transform(scaler.fit_transform(x_test))\n",
    "\n",
    "#outlier detection\n",
    "isolation_forest = IsolationForest(n_estimators=result.x[1], contamination=result.x[2])\n",
    "isolation_forest.fit(x_test_processed)\n",
    "x_test_transformed = x_test_processed[np.where(isolation_forest.predict(x_test_processed) != -1)[0]]\n",
    "\n",
    "#clustering\n",
    "clusterer = hdbscan.HDBSCAN(min_samples=result.x[3], min_cluster_size=result.x[4])\n",
    "cluster_labels = clusterer.fit_predict(x_test_transformed)\n",
    "x_test_clustered = np.concatenate((x_test_transformed, cluster_labels.reshape(-1, 1)), axis=1)\n",
    "\n",
    "#classification\n",
    "gbm = GradientBoostingClassifier(n_estimators=result.x[5], learning_rate=result.x[6], max_depth=result.x[7], random_state=42)\n",
    "gbm.fit(x_test_clustered, y)\n",
    "\n",
    "#predict labels\n",
    "y_pred = gbm.predict(x_test_clustered)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
