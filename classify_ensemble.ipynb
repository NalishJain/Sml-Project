{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier, IsolationForest\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:, 1:-1].values\n",
    "x = np.array(x, dtype=float)\n",
    "y = data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting strings to numeric labels\n",
    "label = 0\n",
    "labels = {}\n",
    "inverse_labels = {}\n",
    "for i in set(y):\n",
    "    labels[i] = label\n",
    "    inverse_labels[label] = i\n",
    "    label += 1\n",
    "\n",
    "for i in range(len(y)):\n",
    "    y[i] = labels[y[i]]\n",
    "\n",
    "y = np.array(y, dtype=float)\n",
    "\n",
    "print(f\"Number of classes: {label}\")\n",
    "pd.DataFrame(y).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [\n",
    "    Integer(1, 20, name='pca_n_components'),\n",
    "    Integer(50, 500, name='gradient_boosting_n_estimators'),\n",
    "    Integer(2, 10, name='gradient_boosting_max_depth'),\n",
    "    Real(10**-4, 10**-1, name='gradient_boosting_learning_rate'),\n",
    "    Integer(50, 500, name='isolation_forest_n_estimators'),\n",
    "    Real(0.1, 0.5, name='isolation_forest_contamination'),\n",
    "    Integer(5, 100, name='hdbscan_min_samples'),\n",
    "    Integer(50, 500, name='hdbscan_min_cluster_size'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(pca_n_components, gradient_boosting_learning_rate, gradient_boosting_n_estimators, gradient_boosting_max_depth, isolation_forest_n_estimators, isolation_forest_contamination, hdbscan_min_samples, hdbscan_min_cluster_size):\n",
    "    #pre-processing\n",
    "    scaler = StandardScaler()\n",
    "    pca = pca(n_components=pca_n_components)\n",
    "    x_processed = pca.fit_transform(scaler.fit_transform(x))\n",
    "\n",
    "    #outlier-detection\n",
    "    isolation_forest = IsolationForest(n_estimators=isolation_forest_n_estimators, contamination=isolation_forest_contamination)\n",
    "    isolation_forest.fit(x)\n",
    "    x_transformed = x_processed[np.where(isolation_forest.predict(x_processed) != -1)[0]]\n",
    "\n",
    "    #clustering\n",
    "    clusterer = hdbscan.HDBSCAN(min_samples=hdbscan_min_samples, min_cluster_size=hdbscan_min_cluster_size)\n",
    "    cluster_labels = clusterer.fit_predict(x_transformed)\n",
    "    x_clustered = np.concatenate((x_transformed, cluster_labels.reshape(-1, 1)), axis=1)\n",
    "\n",
    "    #classification\n",
    "    gbm = GradientBoostingClassifier(n_estimators=gradient_boosting_n_estimators, learning_rate=gradient_boosting_learning_rate, max_depth=gradient_boosting_max_depth, random_state=42)\n",
    "    gbm.fit(x_clustered, y)\n",
    "\n",
    "    #accuracy\n",
    "    return -np.mean(cross_val_score(gbm, x_clustered, y, cv=5, n_jobs=1, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gp_minimize(objective, search_space, n_calls=50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best hyperparameters:', dict(zip(['min_samples_leaf', 'n_estimators', 'max_features'], result.x)))\n",
    "print('Best score:', -result.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = result.x\n",
    "# gbm = GradientBoostingClassifier(n_estimators=best_params[0], learning_rate=best_params[1], max_depth=best_params[2])\n",
    "# gbm.fit(x, y_train)\n",
    "\n",
    "x_test = pd.read_csv('test.csv').values\n",
    "y_pred = gbm.predict(x_test)\n",
    "\n",
    "predicted_categories = []\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    predicted_categories.append(inverse_labels[y_pred[i]])\n",
    "\n",
    "results = pd.DataFrame({'ID': x_test.values[:, 0], 'Category': predicted_categories})\n",
    "results.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
