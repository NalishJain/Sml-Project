{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from sklearn.metrics import accuracy_score\n",
    "import hdbscan\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>n0</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>n3</th>\n",
       "      <th>n4</th>\n",
       "      <th>n5</th>\n",
       "      <th>n6</th>\n",
       "      <th>n7</th>\n",
       "      <th>n8</th>\n",
       "      <th>...</th>\n",
       "      <th>n4087</th>\n",
       "      <th>n4088</th>\n",
       "      <th>n4089</th>\n",
       "      <th>n4090</th>\n",
       "      <th>n4091</th>\n",
       "      <th>n4092</th>\n",
       "      <th>n4093</th>\n",
       "      <th>n4094</th>\n",
       "      <th>n4095</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.272801</td>\n",
       "      <td>0.290501</td>\n",
       "      <td>0.581446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.645888</td>\n",
       "      <td>0.869640</td>\n",
       "      <td>0.302432</td>\n",
       "      <td>0.953719</td>\n",
       "      <td>0.022545</td>\n",
       "      <td>0.498048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034988</td>\n",
       "      <td>0.692382</td>\n",
       "      <td>Orange_Ripe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.542096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.896557</td>\n",
       "      <td>0.049978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117847</td>\n",
       "      <td>...</td>\n",
       "      <td>1.504220</td>\n",
       "      <td>0.622686</td>\n",
       "      <td>0.588427</td>\n",
       "      <td>0.524415</td>\n",
       "      <td>0.305426</td>\n",
       "      <td>0.386204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.668196</td>\n",
       "      <td>Banana_Ripe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098595</td>\n",
       "      <td>0.571866</td>\n",
       "      <td>0.500355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.493137</td>\n",
       "      <td>...</td>\n",
       "      <td>1.169341</td>\n",
       "      <td>0.913239</td>\n",
       "      <td>0.064404</td>\n",
       "      <td>0.531270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.471604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.658250</td>\n",
       "      <td>Mango_Raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101666</td>\n",
       "      <td>1.159194</td>\n",
       "      <td>0.599216</td>\n",
       "      <td>0.893206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200139</td>\n",
       "      <td>0</td>\n",
       "      <td>0.645675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560686</td>\n",
       "      <td>1.243676</td>\n",
       "      <td>0.432523</td>\n",
       "      <td>0.701881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.589985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.591165</td>\n",
       "      <td>Leeche_Raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.178603</td>\n",
       "      <td>0.362568</td>\n",
       "      <td>0.577602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.079862</td>\n",
       "      <td>...</td>\n",
       "      <td>1.206032</td>\n",
       "      <td>0.736831</td>\n",
       "      <td>0.345906</td>\n",
       "      <td>0.878515</td>\n",
       "      <td>0.119000</td>\n",
       "      <td>0.261441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.458905</td>\n",
       "      <td>Mango_Ripe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 4098 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID   n0        n1        n2        n3        n4        n5        n6  n7  \\\n",
       "0   0  0.0  0.000000  1.272801  0.290501  0.581446  0.000000  0.000000   0   \n",
       "1   1  0.0  0.000000  1.542096  0.000000  0.896557  0.049978  0.000000   0   \n",
       "2   2  0.0  0.000000  1.098595  0.571866  0.500355  0.000000  0.000000   0   \n",
       "3   3  0.0  0.101666  1.159194  0.599216  0.893206  0.000000  0.200139   0   \n",
       "4   4  0.0  0.000000  1.178603  0.362568  0.577602  0.000000  0.000000   0   \n",
       "\n",
       "         n8  ...     n4087     n4088     n4089     n4090     n4091     n4092  \\\n",
       "0  0.000000  ...  1.645888  0.869640  0.302432  0.953719  0.022545  0.498048   \n",
       "1  0.117847  ...  1.504220  0.622686  0.588427  0.524415  0.305426  0.386204   \n",
       "2  0.493137  ...  1.169341  0.913239  0.064404  0.531270  0.000000  0.471604   \n",
       "3  0.645675  ...  0.560686  1.243676  0.432523  0.701881  0.000000  0.589985   \n",
       "4  0.079862  ...  1.206032  0.736831  0.345906  0.878515  0.119000  0.261441   \n",
       "\n",
       "   n4093     n4094     n4095     category  \n",
       "0    0.0  0.034988  0.692382  Orange_Ripe  \n",
       "1    0.0  0.000000  0.668196  Banana_Ripe  \n",
       "2    0.0  0.000000  0.658250    Mango_Raw  \n",
       "3    0.0  0.000000  0.591165   Leeche_Raw  \n",
       "4    0.0  0.000000  0.458905   Mango_Ripe  \n",
       "\n",
       "[5 rows x 4098 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:, 1:-1].values\n",
    "x = np.array(x, dtype=float)\n",
    "y = data.iloc[:, -1].values\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Coconut_Raw', 'Banana_Raw', 'Leeche_Ripe', 'Pomengranate_Raw', 'Banana_Ripe', 'Apple_Raw', 'Guava_Ripe', 'Leeche_Raw', 'Guava_Raw', 'Strawberry_Ripe', 'Apple_Ripe', 'Orange_Ripe', 'Mango_Raw', 'Orange_Raw', 'Coconut_Ripe', 'Mango_Ripe', 'Papaya_Raw', 'Pomengranate_Ripe', 'Strawberry_Raw', 'Papaya_Ripe'}\n"
     ]
    }
   ],
   "source": [
    "print(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting strings to numeric labels\n",
    "# label = 0\n",
    "# labels = {}\n",
    "# inverse_labels = {}\n",
    "# for i in set(y):\n",
    "#     labels[i] = label\n",
    "#     inverse_labels[label] = i\n",
    "#     label += 1\n",
    "\n",
    "# for i in range(len(y)):\n",
    "#     y[i] = labels[y[i]]\n",
    "\n",
    "# y = np.array(y, dtype=float)\n",
    "\n",
    "# print(f\"Number of classes: {label}\")\n",
    "# pd.DataFrame(y).head()\n",
    "# print(labels)\n",
    "# print(inverse_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [\n",
    "    Integer(10, 600, name='pca_n_components'),\n",
    "    Integer(1, 18, name='lda_n_components'),\n",
    "    Integer(250, 1000, name='random_forest_n_estimators'),\n",
    "    Integer(10, 50, name='random_forest_max_depth'),\n",
    "    Integer(250, 1000, name='isolation_forest_n_estimators'),\n",
    "    Real(0.1, 0.5, name='isolation_forest_contamination'),\n",
    "    Integer(1, 60, name='hdbscan_min_samples'),\n",
    "    Integer(5, 80, name='hdbscan_min_cluster_size'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    pca_n_components, lda_n_components, random_forest_n_estimators, random_forest_max_depth, isolation_forest_n_estimators, isolation_forest_contamination, hdbscan_min_samples, hdbscan_min_cluster_size = params[0], params[1], params[2], params[3], params[4], params[5], params[6], params[7]\n",
    "    \n",
    "    #pre-processing\n",
    "    scaler = StandardScaler()\n",
    "    pca = PCA(n_components=pca_n_components)\n",
    "    x_pca = pca.fit_transform(scaler.fit_transform(x))\n",
    "\n",
    "    #lda\n",
    "    lda = LinearDiscriminantAnalysis(n_components=lda_n_components)\n",
    "    x_processed = lda.fit_transform(x_pca, y)\n",
    "\n",
    "    #outlier-detection\n",
    "    isolation_forest = IsolationForest(n_estimators=isolation_forest_n_estimators, contamination=isolation_forest_contamination)\n",
    "    isolation_forest.fit(x_processed)\n",
    "    indices = np.where(isolation_forest.predict(x_processed) != -1)[0]\n",
    "    x_cleaned = x_processed[indices]\n",
    "    y_cleaned = y[indices]\n",
    "\n",
    "    #clustering\n",
    "    clusterer = hdbscan.HDBSCAN(min_samples=hdbscan_min_samples, min_cluster_size=hdbscan_min_cluster_size)\n",
    "    cluster_labels = clusterer.fit_predict(x_cleaned)\n",
    "    x_clustered = np.concatenate((x_cleaned, cluster_labels.reshape(-1, 1)), axis=1)\n",
    "\n",
    "    #classification\n",
    "    rf = RandomForestClassifier(n_estimators=random_forest_n_estimators, max_depth=random_forest_max_depth)\n",
    "\n",
    "    #accuracy\n",
    "    return -np.mean(cross_val_score(rf, x_clustered, y_cleaned, cv=5, n_jobs=-1, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = gp_minimize(objective, search_space, n_calls=100, random_state=42, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Best hyperparameters:', result.x)\n",
    "# print('Best score:', -result.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.908889   ... 0.         0.         0.65567034]\n",
      " [0.         0.         1.1910553  ... 0.         0.         0.61449343]\n",
      " [0.         0.26190305 0.99278164 ... 0.         0.         0.39215815]\n",
      " [0.         0.         1.35240054 ... 0.         0.         0.62836468]\n",
      " [0.         0.         1.11428118 ... 0.         0.         0.83567119]]\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('test.csv')\n",
    "x_test = data2.iloc[:, 1:].values\n",
    "x_test = np.array(x_test, dtype='float')\n",
    "print(x_test[:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #pre-processing\n",
    "# scaler1 = StandardScaler()\n",
    "# scaler1.fit(x)\n",
    "# pca1 = PCA(n_components=result.x[0])\n",
    "# pca1.fit(x)\n",
    "# lda = LinearDiscriminantAnalysis(n_components=result.x[1])\n",
    "# lda.fit(x, y)\n",
    "# x_processed = lda.transform(pca1.transform(scaler1.transform(x)))\n",
    "# x_test_processed = lda.transform(pca1.transform(scaler1.transform(x_test)))\n",
    "\n",
    "# #outlier-detection\n",
    "# isolation_forest = IsolationForest(n_estimators=result.x[4], contamination=result.x[5])\n",
    "# isolation_forest.fit(x_processed)\n",
    "# indices = np.where(isolation_forest.predict(x) != -1)[0]\n",
    "# x_cleaned = x[indices]\n",
    "# y_cleaned = y[indices]\n",
    "\n",
    "# #clustering\n",
    "# clusterer1 = hdbscan.HDBSCAN(min_samples=result.x[6], min_cluster_size=result.x[7])\n",
    "# cluster_labels1 = clusterer1.fit_predict(x_cleaned)\n",
    "# x_clustered = np.concatenate((x_cleaned, cluster_labels1.reshape(-1, 1)), axis=1)\n",
    "\n",
    "# clusterer2 = hdbscan.HDBSCAN(min_samples=result.x[6], min_cluster_size=result.x[7])\n",
    "# cluster_labels2 = clusterer2.fit_predict(x_test_processed)\n",
    "# x_test_clustered = np.concatenate((x_test_processed, cluster_labels2.reshape(-1, 1)), axis=1)\n",
    "\n",
    "# # x_train, x_validate, y_train, y_validate = train_test_split(x_clustered, y_transformed, test_size=0.3, random_state=1)\n",
    "# # print(set(x_test_clustered[:, -1]))\n",
    "# #classification\n",
    "# rf = RandomForestClassifier(n_estimators=result.x[2], max_depth=result.x[3])\n",
    "\n",
    "# # print(labels)\n",
    "# # print(inverse_labels)\n",
    "# rf.fit(x_clustered, y_cleaned)\n",
    "\n",
    "# #predict labels\n",
    "# y_pred = rf.predict(x_test_clustered)\n",
    "\n",
    "# # predicted_categories = []\n",
    "# # print(y_pred)\n",
    "\n",
    "# # for i in range(len(y_pred)):\n",
    "# #     predicted_categories.append(inverse_labels[y_pred[i]])\n",
    "\n",
    "# ids = range(415)\n",
    "# # print(predicted_categories)\n",
    "# previous_submission = pd.read_csv(\"submission.csv\")\n",
    "# results = pd.DataFrame({'ID': ids, 'Category': y_pred})\n",
    "# results.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8930543337103598\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rajad\\OneDrive\\Documents\\GitHub\\SmlProject\\classify_ensemble.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rajad/OneDrive/Documents/GitHub/SmlProject/classify_ensemble.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# print(labels)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rajad/OneDrive/Documents/GitHub/SmlProject/classify_ensemble.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# print(inverse_labels)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rajad/OneDrive/Documents/GitHub/SmlProject/classify_ensemble.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39mmean(cross_val_score(rf, x_clustered, y_cleaned, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rajad/OneDrive/Documents/GitHub/SmlProject/classify_ensemble.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m rf\u001b[39m.\u001b[39;49mfit(x_clustered, y_cleaned)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rajad/OneDrive/Documents/GitHub/SmlProject/classify_ensemble.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m#predict labels\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rajad/OneDrive/Documents/GitHub/SmlProject/classify_ensemble.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m y_pred \u001b[39m=\u001b[39m rf\u001b[39m.\u001b[39mpredict(x_test_clustered)\n",
      "File \u001b[1;32mc:\\Users\\rajad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    474\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    475\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    476\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    477\u001b[0m )(\n\u001b[0;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    479\u001b[0m         t,\n\u001b[0;32m    480\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    481\u001b[0m         X,\n\u001b[0;32m    482\u001b[0m         y,\n\u001b[0;32m    483\u001b[0m         sample_weight,\n\u001b[0;32m    484\u001b[0m         i,\n\u001b[0;32m    485\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    486\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    487\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    488\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    489\u001b[0m     )\n\u001b[0;32m    490\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\rajad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\rajad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rajad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rajad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\rajad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\rajad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\rajad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\rajad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\rajad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rajad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    182\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 184\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\rajad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    890\u001b[0m         X,\n\u001b[0;32m    891\u001b[0m         y,\n\u001b[0;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rajad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#pre-processing\n",
    "scaler1 = StandardScaler()\n",
    "pca1 = PCA(n_components=200)\n",
    "p1 = pca1.fit(scaler1.fit_transform(x))\n",
    "x_pca = p1.transform(x)\n",
    "xt_pca = p1.transform(x_test)\n",
    "lda = LinearDiscriminantAnalysis(n_components=19, priors=None, shrinkage=None, solver='svd', store_covariance=False, tol=0.0001)\n",
    "l1 = lda.fit(x_pca, y)\n",
    "x_processed = l1.transform(x_pca)\n",
    "x_test_processed = l1.transform(xt_pca)\n",
    "\n",
    "#outlier-detection\n",
    "isolation_forest = IsolationForest(n_estimators=500, contamination=0.1)\n",
    "isolation_forest.fit(x_processed)\n",
    "indices = np.where(isolation_forest.predict(x_processed) != -1)[0]\n",
    "x_cleaned = x_processed[indices]\n",
    "y_cleaned = y[indices]\n",
    "\n",
    "#clustering\n",
    "clusterer1 = hdbscan.HDBSCAN(min_samples=1, min_cluster_size=20)\n",
    "cluster_labels1 = clusterer1.fit_predict(x_cleaned)\n",
    "x_clustered = np.concatenate((x_cleaned, cluster_labels1.reshape(-1, 1)), axis=1)\n",
    "\n",
    "clusterer2 = hdbscan.HDBSCAN(min_samples=1, min_cluster_size=20)\n",
    "cluster_labels2 = clusterer2.fit_predict(x_test_processed)\n",
    "x_test_clustered = np.concatenate((x_test_processed, cluster_labels2.reshape(-1, 1)), axis=1)\n",
    "\n",
    "# x_train, x_validate, y_train, y_validate = train_test_split(x_clustered, y_transformed, test_size=0.3, random_state=1)\n",
    "# print(set(x_test_clustered[:, -1]))\n",
    "#classification\n",
    "rf = RandomForestClassifier(n_estimators=1000, max_depth=17)\n",
    "\n",
    "# print(labels)\n",
    "# print(inverse_labels)\n",
    "print(np.mean(cross_val_score(rf, x_clustered, y_cleaned, cv=5)))\n",
    "rf.fit(x_clustered, y_cleaned)\n",
    "\n",
    "#predict labels\n",
    "y_pred = rf.predict(x_test_clustered)\n",
    "\n",
    "# predicted_categories = []\n",
    "# print(y_pred)\n",
    "\n",
    "# for i in range(len(y_pred)):\n",
    "#     predicted_categories.append(inverse_labels[y_pred[i]])\n",
    "\n",
    "ids = range(415)\n",
    "# print(predicted_categories)\n",
    "previous_submission = pd.read_csv(\"submission.csv\")\n",
    "previous_submission.to_csv('prev.csv', index=False)\n",
    "results = pd.DataFrame({'ID': ids, 'Category': y_pred})\n",
    "results.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
